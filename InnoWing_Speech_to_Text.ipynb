{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c84b0b"
      },
      "source": [
        "# Speech-to-Text (STT) with OpenAI Whisper\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nontgcob/HKU-InnoWing-STT-TTS-Workshop/blob/main/InnoWing_Speech_to_Text.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use the OpenAI Whisper model for Speech-to-Text (STT) transcription. We will download a 'fast' version of the model, load it, and then perform inference on an audio file.\n",
        "\n",
        "Note: Speech-to-Text (STT) is sometimes referred to as Automatic Speech Recognition (ASR). These two terms can be used interchangeably."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48e1d253"
      },
      "source": [
        "# The first step is to install the necessary library: `openai-whisper`.\n",
        "# The `-q` flag ensures a quiet installation, meaning it won't print all the installation details.\n",
        "!pip install -q openai-whisper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f15f315"
      },
      "source": [
        "Whisper often relies on `ffmpeg` for audio processing. Ensure it's installed in your Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c293031"
      },
      "source": [
        "# First, we update the list of available packages.\n",
        "!apt-get update\n",
        "# Then, we install ffmpeg. The `-y` flag automatically confirms any prompts.\n",
        "!apt-get install -y ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bbdda9"
      },
      "source": [
        "### 1. Load the Whisper Model\n",
        "\n",
        "We will load the `base` model, which is a good balance between speed and accuracy for a 'fast' version. You can choose other models like `tiny`, `small`, `medium`, or `large` depending on your needs. Smaller models are faster but less accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07f0fad"
      },
      "source": [
        "import whisper # Import the Whisper library to use its functions\n",
        "\n",
        "# Choose the model size. 'base' is chosen here for a good balance of speed and accuracy.\n",
        "# You can try 'tiny', 'small', 'medium', or 'large' if you wish.\n",
        "# For English-only transcription, you could use 'base.en' for slightly better performance.\n",
        "model_name = \"base\"\n",
        "\n",
        "# Load the selected Whisper model into memory.\n",
        "# This step downloads the model weights the first time you run it.\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "print(f\"Whisper model '{model_name}' loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76b343ea"
      },
      "source": [
        "### 2.1 Perform Inference: Transcribe from Audio File\n",
        "\n",
        "Now, let's transcribe the audio file using the loaded Whisper model.\n",
        "\n",
        "The code cell below transcribes an audio file. It is commented out for the workshop's focus on microphone input.\n",
        "\n",
        "If you have your own audio file (e.g. `my_audio.wav`) and wish to transcribe it, uncomment the entire code block below and ensure `my_audio.wav` is replaced with your actual audio file name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60891a95"
      },
      "source": [
        "# print(\"Transcribing audio from file...\")\n",
        "# user_audio_path = \"my_audio.mp3\" # <--- Change this to your audio file's path\n",
        "# result = model.transcribe(user_audio_path)\n",
        "\n",
        "# print(\"\\n--- Transcription Result from File ---\")\n",
        "# print(result[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9506ddf2"
      },
      "source": [
        "### 2.2. Perform Inference: Transcribe from Browser Microphone\n",
        "\n",
        "To use your browser's microphone, we'll leverage some `IPython.display` utilities to create a simple recording interface directly in the notebook. The recorded audio will then be processed and transcribed by the Whisper model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02e32003"
      },
      "source": [
        "from IPython.display import display, Javascript, Audio # Tools for displaying rich content in Colab\n",
        "from google.colab import output # Used to register a Python function as a callback for JavaScript\n",
        "import base64 # For encoding/decoding binary data (our audio) to/from text\n",
        "from scipy.io.wavfile import write # To save the recorded audio as a WAV file\n",
        "import threading # Used to help Python 'wait' for the asynchronous JavaScript recording to finish\n",
        "\n",
        "# This is a special tool (an 'event' object) that helps synchronize tasks.\n",
        "# We use it to make sure our Python code waits until the browser has finished recording and saving the audio.\n",
        "recording_done_event = threading.Event()\n",
        "\n",
        "def record_audio(duration, filename='microphone_audio.wav', samplerate=16000): # 'duration' is now the first parameter for clarity\n",
        "    global model # Allows us to use the Whisper 'model' loaded earlier in this function\n",
        "    global recording_done_event # Allows us to use our synchronization event\n",
        "\n",
        "    # Clear the event. This prepares it to 'wait' again for a new recording.\n",
        "    recording_done_event.clear()\n",
        "\n",
        "    # This is a block of JavaScript code that will run in your web browser.\n",
        "    # It requests microphone access, records audio for a specified duration,\n",
        "    # and then sends the recorded audio (as base64 encoded text) back to Python.\n",
        "    JS_CODE = f'''\n",
        "        async function recordAudio() {{\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }}); // Request microphone access\n",
        "            const mediaRecorder = new MediaRecorder(stream); // Create a recorder\n",
        "            const audioChunks = []; // Array to store parts of the audio\n",
        "\n",
        "            mediaRecorder.addEventListener('dataavailable', event => {{\n",
        "                audioChunks.push(event.data); // Add audio data as it becomes available\n",
        "            }});\n",
        "\n",
        "            const audioPromise = new Promise(resolve => {{\n",
        "                mediaRecorder.addEventListener('stop', () => {{\n",
        "                    // When recording stops, combine the audio chunks into a Blob\n",
        "                    const audioBlob = new Blob(audioChunks, {{ 'type' : 'audio/wav' }});\n",
        "                    const reader = new FileReader();\n",
        "                    reader.onloadend = () => {{ resolve(reader.result.split(',')[1]); }}; // Convert Blob to base64\n",
        "                    reader.readAsDataURL(audioBlob);\n",
        "                }});\n",
        "            }});\n",
        "\n",
        "            mediaRecorder.start(); // Start recording\n",
        "            await new Promise(resolve => setTimeout(resolve, {duration * 1000})); // Record for 'duration' seconds\n",
        "            mediaRecorder.stop(); // Stop recording\n",
        "\n",
        "            return await audioPromise; // Return the base64 encoded audio\n",
        "        }}\n",
        "\n",
        "        // Call the recordAudio function and then invoke our Python callback with the audio data\n",
        "        recordAudio().then(audioBase64 => google.colab.kernel.invokeFunction('notebook_record_audio_callback', [audioBase64]));\n",
        "    '''\n",
        "\n",
        "    # Display the JavaScript code, which executes it in your browser.\n",
        "    display(Javascript(JS_CODE))\n",
        "    print(f\"Recording for {duration} seconds...\")\n",
        "\n",
        "    # This nested function is a Python 'callback'. It's designed to be called by the JavaScript\n",
        "    # code running in your browser, *after* the audio recording is complete.\n",
        "    def _record_audio_callback(audio_base64):\n",
        "        # Decode the base64 audio data received from JavaScript back into raw bytes\n",
        "        audio_bytes = base64.b64decode(audio_base64)\n",
        "        # Save these audio bytes to a WAV file on our Colab environment's disk\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(audio_bytes)\n",
        "        print(f\"Audio saved to {filename}\")\n",
        "\n",
        "        # Now that the audio is saved, we can proceed with transcribing it using Whisper\n",
        "        print(\"Transcribing recorded microphone audio...\")\n",
        "        result_mic = model.transcribe(filename)\n",
        "\n",
        "        print(\"\\n--- Transcription Result from Microphone ---\")\n",
        "        print(result_mic[\"text\"])\n",
        "\n",
        "        # As a bonus, we can play back the recorded audio directly in the notebook\n",
        "        print(\"\\n--- Playing back recorded audio ---\")\n",
        "        display(Audio(filename))\n",
        "\n",
        "        # Signal that all recording and processing steps are now complete\n",
        "        recording_done_event.set()\n",
        "\n",
        "    # Register our Python callback function so that the JavaScript code can call it.\n",
        "    # This needs to be done each time `record_audio` is called in a new execution,\n",
        "    # as the Colab kernel resets callback registrations.\n",
        "    output.register_callback('notebook_record_audio_callback', _record_audio_callback)\n",
        "\n",
        "    # The Python code will now 'wait' here until the 'recording_done_event' is set\n",
        "    # by our callback function (meaning the recording and transcription are done).\n",
        "    # We add a generous timeout to ensure the process has enough time to complete.\n",
        "    timeout_seconds = duration + 10 # Adding 10 seconds buffer for recording and processing\n",
        "    if not recording_done_event.wait(timeout=timeout_seconds):\n",
        "        print(f\"Warning: Recording or transcription timed out after {timeout_seconds} seconds. This usually happens if microphone permissions were not granted or there's a browser issue. Please check your microphone and browser permissions and try again.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d896a169"
      },
      "source": [
        "# Define the recording duration in seconds\n",
        "recording_duration_seconds = 5\n",
        "\n",
        "# Record audio for the specified duration\n",
        "mic_audio_path = \"microphone_audio.wav\"\n",
        "record_audio(duration=recording_duration_seconds, filename=mic_audio_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
